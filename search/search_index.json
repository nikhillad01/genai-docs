{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"GenAI Docs","text":""},{"location":"#modules-to-explore","title":"Modules to Explore","text":"<p>Here\u2019s where the fun begins! Choose any module to start learning:</p>"},{"location":"#foundations","title":"Foundations","text":"<p>Just getting started? This is where you\u2019ll find the basics of AI, LangChain, and prompt engineering.</p> <ul> <li>What\u2019s Generative AI?</li> <li>Working with LLMs</li> <li>Prompt Engineering 101</li> <li>AI Tools Overview</li> </ul>"},{"location":"#getting-hands-on-with-langchain","title":"Getting Hands-On with LangChain","text":"<p>Ready to get your hands dirty? This module is all about practical, real-world examples with LangChain.</p> <ul> <li>LangChain Intro</li> <li>Tutorials</li> <li>LangChain in Action</li> </ul>"},{"location":"#advanced-langchain-integrations","title":"Advanced LangChain + Integrations","text":"<p>Now you\u2019re ready to take things up a notch! Dive into advanced LangChain techniques and integrations with other tools.</p> <ul> <li>Deep Dive into LangChain</li> <li>Advanced Techniques</li> <li>Real-World Use Cases</li> </ul>"},{"location":"#productionizing-real-projects","title":"Productionizing + Real Projects","text":"<p>Ready to go beyond the basics? Learn how to take your AI projects to production.</p> <ul> <li>Going Live</li> <li>Best Practices</li> <li>Real-World Projects</li> </ul>"},{"location":"#logging-tracing-debugging","title":"Logging, Tracing, Debugging","text":"<p>Debugging is where the magic happens! Learn all about logging and tracing to make your projects rock-solid.</p> <ul> <li>Intro to Debugging</li> <li>Logging &amp; Tracing Techniques</li> <li>Top Debugging Tools</li> </ul>"},{"location":"#lets-get-started","title":"Let\u2019s Get Started!","text":"<p>Pick a module, dive in, and start building! Whether you're just exploring or ready to build the next big thing, we\u2019ve got everything you need right here.</p>"},{"location":"foundations/LLMs/","title":"LLMs","text":""},{"location":"foundations/LLMs/#llms-large-language-models","title":"LLMs (Large Language Models)","text":"<p>LLMs are deep learning models trained on massive amounts of text data to understand and generate human-like language. They can perform tasks like text generation, translation, summarization, and question answering.</p> <p>Examples</p> <ul> <li>GPT (Generative Pretrained Transformer) \u2013 Text generation, conversational AI.</li> <li>BERT (Bidirectional Encoder Representations from Transformers) \u2013 Text understanding, sentiment analysis.</li> <li>T5 (Text-to-Text Transfer Transformer) \u2013 Multi-task text generation and transformation.</li> </ul>"},{"location":"foundations/LLMs/#vectors","title":"Vectors","text":"<p>In machine learning and AI, a vector is a list (or array) of numerical values. These numbers represent a point in a multi-dimensional space.</p> <ul> <li>Each number in the vector is called a dimension.</li> <li>For example, a vector could be <code>[2, 5, 3]</code>, which has 3 dimensions (x, y, and z).</li> </ul> <p>In the case of word embeddings, a word like \"cat\" is represented as a vector, where each number in the vector reflects a feature of the word, such as its meaning, context, or relationship to other words.</p>"},{"location":"foundations/LLMs/#why-we-use-vectors","title":"Why We Use Vectors","text":"<p>Vectors allow us to:</p> <ol> <li>Numerically represent words or data: Words themselves are not numbers, but by converting them to vectors, we make them usable in mathematical models.</li> <li>Capture relationships and similarities: The distance between two vectors can show how similar two words are. For example, the vector for \"king\" might be close to the vector for \"queen\" because they share similar meanings.</li> </ol>"},{"location":"foundations/LLMs/#example-of-a-vector-representation","title":"Example of a Vector Representation","text":"<p>A word like \"apple\" might be represented as a vector like <code>[0.1, 0.2, 0.3, 0.4]</code>, where: - <code>0.1</code> might correspond to the word's fruit-related meaning. - <code>0.2</code> could represent its sweetness. - <code>0.3</code> might capture its association with technology (like Apple Inc.). - <code>0.4</code> could relate to color or shape.</p> <p>Each dimension in the vector represents a feature that helps the model understand the word better.</p>"},{"location":"foundations/LLMs/#why-vectors-are-important","title":"Why Vectors Are Important","text":"<p>Vectors allow models to: - Process words as numbers, making it easier to apply mathematical operations (like calculating similarities or differences). - Capture context: A vector for the word \"bank\" in a financial context will be different from one in a riverbank context, allowing models to handle polysemy (words with multiple meanings).</p>"},{"location":"foundations/LLMs/#how-vectors-capture-word-context","title":"How Vectors Capture Word Context","text":""},{"location":"foundations/LLMs/#meaning-of-vectors-in-context","title":"Meaning of Vectors in Context","text":"<p>When we convert words into vectors (using embeddings), each word gets a numerical representation that captures its meaning based on context. This is achieved by training the model on large amounts of text where it learns how words are used in different situations.</p> <ul> <li>Context matters: If the word \"orange\" is used in a sentence like \"I ate an orange,\" the model learns that \"orange\" refers to a fruit. If it\u2019s used in \"The sky is orange,\" the model learns that it refers to a color.</li> </ul>"},{"location":"foundations/LLMs/#how-context-is-captured","title":"How Context is Captured","text":"<p>Embeddings are generated using techniques like Word2Vec or GloVe. These methods assign words to vectors based on their co-occurrence with other words in different contexts.</p> <p>Example 1: Word2Vec (Skip-Gram Model)</p> <ul> <li>When the word \"orange\" is used in a sentence like \"I ate an orange,\" the model will learn that \"orange\" often appears near words like fruit, delicious, eat.</li> <li>When used in \"The sky is orange,\" it will often appear near words like color, sunset, sky.</li> </ul> <p>The model will generate different vectors for \"orange\" in these contexts because of the surrounding words. These vectors will reflect the specific context of the word in each case.</p> <p>Example 2: GloVe (Global Vectors for Word Representation)</p> <ul> <li>GloVe looks at the global context of words across an entire corpus. It learns that \"orange\" shares different co-occurrence statistics with words like fruit and color.</li> <li>The resulting vectors will still be close to each other, but they\u2019ll be distinct enough to represent different meanings in different contexts.</li> </ul>"},{"location":"foundations/LLMs/#dimensionality-and-contextual-representation","title":"Dimensionality and Contextual Representation","text":"<p>Each vector typically has many dimensions (e.g., 300-dimensional). Each dimension doesn\u2019t directly correspond to a specific meaning like \"sweetness\" or \"color,\" but rather represents latent features learned from data. These features emerge from how words relate to other words in sentences.</p> <p>For Example: - Context 1: \"I ate an orange\" \u2013 The vector might emphasize features related to edible, fruit, and taste. - Context 2: \"The sunset was orange\" \u2013 The vector might emphasize features related to color, sky, and nature.</p>"},{"location":"foundations/LLMs/#word-sense-disambiguation-wsd","title":"Word Sense Disambiguation (WSD)","text":"<p>Models like BERT or GPT are designed to handle this. They consider the full sentence (or even larger context) to adjust the vector of a word dynamically. This process is called contextual embedding, where the same word might have slightly different vector representations depending on its usage.</p>"},{"location":"foundations/LLMs/#why-vectors-capture-context","title":"Why Vectors Capture Context","text":"<p>By using large corpora of text, embeddings capture the statistical relationship between words in various contexts. This allows models to differentiate meanings like: - \"Orange\" as a fruit when surrounded by words like \"eat,\" \"juice,\" and \"sweet.\" - \"Orange\" as a color when surrounded by words like \"sky,\" \"sunset,\" and \"bright.\"</p> <p>Thus, vectors represent both the general meaning of the word and its specific meaning in context.</p>"},{"location":"foundations/LLMs/#tokenization-embeddings-and-transformers","title":"Tokenization, Embeddings, and Transformers","text":""},{"location":"foundations/LLMs/#tokenization","title":"Tokenization","text":"<p>Tokenization is the process of breaking down text into smaller pieces, such as words, subwords, or characters. These tokens are then fed into models for processing.</p> <ul> <li> <p>Why We Need It: Tokenization allows the model to handle text in manageable units. It\u2019s necessary because the model needs a consistent format to understand and process varying-length input.</p> </li> <li> <p>Example: \"I love AI\" becomes <code>['I', 'love', 'AI']</code>.</p> </li> </ul>"},{"location":"foundations/LLMs/#embeddings","title":"Embeddings","text":"<p>Embeddings are dense vector representations of tokens. Each token is mapped to a vector of numbers that capture its meaning based on context.</p> <ul> <li> <p>Why We Need It: Embeddings allow words to be represented in a way that preserves their semantic meaning. Unlike traditional one-hot encoding, which only shows whether a word exists or not, embeddings capture context and relationships between words.</p> </li> <li> <p>Word Embedding Example: The word \"cat\" might be represented as <code>[0.1, 0.2, 0.3, ...]</code> in a multi-dimensional space.</p> </li> </ul>"},{"location":"foundations/LLMs/#transformers","title":"Transformers","text":"<p>Transformers are a type of neural network architecture that processes input data in parallel rather than sequentially, making them highly efficient. They use self-attention mechanisms to focus on different parts of the input sequence for better context understanding.</p> <ul> <li> <p>Why We Need It:  Transformers are crucial because they handle long-range dependencies in text and process data much faster than older models like RNNs. Self-attention helps the model focus on relevant parts of the input sequence, even if they are far apart in the text.</p> </li> <li> <p>Key Concepts:</p> <ul> <li>Self-Attention: Allows the model to consider all parts of the input sequence at once, making it more efficient for tasks requiring context from multiple parts of the text.</li> <li>Positional Encoding: Adds information about the order of tokens, ensuring the model understands the sequence of the input.</li> </ul> </li> <li> <p>Popular Transformers:</p> <ul> <li>GPT: Focuses on autoregressive text generation.</li> <li>BERT: Focuses on bidirectional context understanding.</li> </ul> </li> </ul>"},{"location":"foundations/LLMs/#semantic-search","title":"Semantic Search","text":"<p>Semantic search finds results based on meaning, not just keywords. It uses embeddings (vectors) to compare the intent behind a query and stored content.</p> <ul> <li>Why we need it:   Traditional search (SQL or keyword-based) only matches exact words. Semantic search finds the most relevant content even if the exact words don't match.</li> </ul>"},{"location":"foundations/LLMs/#example","title":"Example","text":"<p>User query: <code>\"How do users log in?\"</code> Semantic search can match this with docs containing: - \"User authentication flow\" - \"OAuth login handler\" - \"Sign-in endpoint in API\"</p> <p>Even though the words differ, the meaning is close.</p>"},{"location":"foundations/LLMs/#how-it-works-high-level","title":"How it works (high-level):","text":"<ol> <li>Convert the query and documents into embeddings (vectors).</li> <li>Use a Vector DB to find content with the most similar vectors.</li> <li>Pass those results to the LLM to generate the final answer.</li> </ol>"},{"location":"foundations/intro/","title":"Introduction","text":""},{"location":"foundations/intro/#generative-ai","title":"Generative AI","text":"<p>Generative AI is a type of artificial intelligence that can create new content such as text, images, music, or code by learning patterns from existing data.</p> <p>Examples</p> <ul> <li>ChatGPT \u2013 generates human-like text.</li> <li>DALL\u00b7E \u2013 generates images from text.</li> <li>GitHub Copilot \u2013 suggests and generates code.</li> </ul>"},{"location":"foundations/intro/#insight","title":"Insight","text":"<p>Traditional AI focuses on tasks like classification and prediction. Generative AI is different\u2014it creates new data, which opens up creative and automation possibilities.</p>"},{"location":"foundations/intro/#types-of-generative-models","title":"Types of Generative Models","text":"<p>Examples</p> <ul> <li>GPT (Generative Pretrained Transformer) \u2013 Text generation</li> <li>DALL\u00b7E / Stable Diffusion \u2013 Image generation</li> <li>StyleGAN \u2013 Face and image generation</li> <li>VAE (Variational Autoencoder) \u2013 Image compression + generation</li> <li>RNN / LSTM \u2013 Older models for text and music generation</li> </ul>"},{"location":"foundations/intro/#insight_1","title":"Insight","text":"<p>Each model has its strengths: - Transformers (e.g., GPT) are great for language and sequential data. - GANs are excellent for generating realistic images. - VAEs are used for controlled, smooth variation in generated data.</p>"},{"location":"foundations/intro/#connection","title":"Connection","text":"<p>As a dev, knowing the types helps you choose: - Use GPT-like models for chatbots, code generation. - Use GANs/VAEs for image-related tasks. - Use diffusion models for high-quality, customizable image generation.</p>"},{"location":"foundations/prompt_engineering/","title":"Prompt Engineering Basics","text":""},{"location":"foundations/prompt_engineering/#zero-shot-prompting","title":"Zero-shot Prompting","text":"<p>Ask the model to perform a task with no examples.</p> <ul> <li> <p>Example:   \"Write a Python function to sort a list of integers.\"</p> </li> <li> <p>Why it's useful:   Good for simple, well-understood tasks where the model can rely on its pretraining.</p> </li> </ul>"},{"location":"foundations/prompt_engineering/#one-shot-prompting","title":"One-shot Prompting","text":"<p>Provide one example to guide the model's behavior.</p> <ul> <li> <p>Example:   \"Convert a Python list to a JSON string.   Input: [1, 2, 3]   Output: '[1, 2, 3]'   Input: ['a', 'b', 'c']   Output:\"</p> </li> <li> <p>Why it's useful:   Clarifies the expected input-output format, especially when ambiguity exists.</p> </li> </ul>"},{"location":"foundations/prompt_engineering/#few-shot-prompting","title":"Few-shot Prompting","text":"<p>Provide 2\u20135 examples to show the pattern clearly.</p> <ul> <li> <p>Example:   \"Convert a Python snippet to equivalent JavaScript.   Python: <code>print('Hello')</code>   JS: <code>console.log('Hello');</code>   Python: <code>len([1,2,3])</code>   JS: <code>[1,2,3].length</code>   Python: <code>range(5)</code>   JS:\"</p> </li> <li> <p>Why it's useful:   Boosts accuracy by showing consistent translation logic across multiple examples.</p> </li> </ul>"},{"location":"foundations/prompt_engineering/#prompt-chaining","title":"Prompt Chaining","text":"<p>Break down a complex task into smaller sub-prompts, passing output from one step to the next.</p> <ul> <li>Example: Generating API documentation</li> <li>Prompt 1: \"Summarize what this FastAPI function does.\"</li> <li>Prompt 2: \"Generate an OpenAPI description from the summary.\"</li> <li> <p>Prompt 3: \"Convert OpenAPI to Markdown format.\"</p> </li> <li> <p>Why it's useful:   Keeps each step focused and manageable. Helps reduce hallucination and improves control over complex workflows.</p> </li> </ul>"},{"location":"foundations/tools_overview/","title":"Tools Overview","text":""},{"location":"foundations/tools_overview/#langchain","title":"LangChain","text":"<p>LangChain is a framework designed to help developers build applications that use LLMs (like GPT-4) in a structured and effective way. It provides pre-built abstractions and components for integrating LLMs into your applications.</p> <p>Think of LangChain as React for LLM-based apps. It provides a set of building blocks to help create powerful language-based systems such as chatbots, agents, retrieval-based apps, or code assistants.</p>"},{"location":"foundations/tools_overview/#why-do-we-need-langchain","title":"Why Do We Need LangChain?","text":"<p>While using LLMs directly (via OpenAI API or similar) works for simple use cases, when you need:</p> <ul> <li>Multiple steps (reasoning, memory, context handling),</li> <li>Combining tools (e.g., search, databases, calculators),</li> <li>Chaining prompts together,</li> <li>Handling structured outputs,</li> <li>Calling APIs or tools conditionally,</li> </ul> <p>LangChain offers an orchestration layer that simplifies these tasks. Without it, writing this from scratch would be complex and time-consuming.</p>"},{"location":"foundations/tools_overview/#core-concepts-in-langchain","title":"Core Concepts in LangChain","text":"<p>Here\u2019s a breakdown of the key components of LangChain:</p> <p>LLMs</p> <ul> <li>What: Interface to large language models (like OpenAI, Anthropic, HuggingFace).</li> <li>Why: Provides a unified way to access multiple model providers with a consistent API.</li> </ul> <p>Prompts</p> <ul> <li>What: Prompt templates with placeholders (<code>PromptTemplate</code>).</li> <li>Why: Allows reuse and maintenance of clean, parameterized prompts across different tasks.</li> </ul> <p>Chains</p> <ul> <li>What: A sequence of components (e.g., LLM, prompt, output parsers).</li> <li>Why: Encapsulates complex logic into a reusable pipeline.</li> <li>Example: Question \u2192 Prompt \u2192 LLM \u2192 Answer</li> </ul> <p>Tools</p> <ul> <li>What: External functions or APIs that the LLM can call (e.g., Google search, database query).</li> <li>Why: Enables LLMs to interact with the external world (not just generate text).</li> </ul> <p>Agents</p> <ul> <li>What: LLMs that decide what tool to use and act step-by-step to achieve a goal.</li> <li>Why: Ideal for dynamic reasoning tasks like \u201ccheck the weather and report if it's raining.\u201d</li> </ul>"},{"location":"foundations/tools_overview/#langchain-use-cases","title":"LangChain Use Cases","text":"<p>LangChain helps with applications that require advanced chaining and decision-making. Examples include:</p> <ol> <li>Conversational Agents: Build intelligent chatbots that can access external tools and databases.</li> <li>Document Retrieval: Create systems that search documents and answer questions based on content.</li> <li>Code Generation &amp; Assistance: Use LLMs for dynamic code generation by combining different models and tools.</li> </ol>"},{"location":"foundations/tools_overview/#llamaindex","title":"LlamaIndex","text":"<p>A Framework for Building Retrieval-Augmented Generation (RAG) Applications</p> <p>LlamaIndex (formerly known as GPT Index) is a framework designed for creating retrieval-augmented generation (RAG) applications. It simplifies the process of connecting a language model (like GPT) to external data sources, enabling the model to retrieve relevant information and generate responses based on that information.</p>"},{"location":"foundations/tools_overview/#why-do-we-need-llamaindex","title":"Why Do We Need LlamaIndex?","text":"<p>When working with large language models (LLMs) like GPT, they are typically trained on a fixed set of data. This means that any information that is beyond the training data or more recent will not be available for the model. This is where retrieval-augmented generation comes in \u2014 it allows LLMs to dynamically retrieve data from external sources (such as databases, documents, or APIs) to generate more accurate, contextually relevant, and up-to-date responses.</p> <p>LlamaIndex helps facilitate this by providing a simple interface for connecting LLMs to data sources and performing the retrieval process efficiently.</p>"},{"location":"foundations/tools_overview/#core-concepts-in-llamaindex","title":"Core Concepts in LlamaIndex","text":"<p>Here are the main components of LlamaIndex:</p> <p>Data Connectors</p> <ul> <li>What: Interfaces that connect external data sources to LlamaIndex (e.g., databases, documents, APIs).</li> <li>Why: LlamaIndex allows the LLM to access real-time and dynamic information by integrating these data connectors.</li> </ul> <p>Indexing</p> <ul> <li>What: The process of creating an index for efficient retrieval of relevant information from the connected data sources.</li> <li>Why: Indexing allows for fast and efficient searching of large amounts of data, improving the overall speed and accuracy of the retrieval process.</li> </ul> <p>Retrievers</p> <ul> <li>What: Components responsible for retrieving relevant information from the indexed data sources.</li> <li>Why: Ensures that the model fetches the most relevant data based on the query, increasing the quality of the generated responses.</li> </ul> <p>LLMs</p> <ul> <li>What: The language model used to generate responses, typically using the retrieved information.</li> <li>Why: LlamaIndex leverages LLMs to generate contextually-aware and accurate responses based on the retrieved data.</li> </ul> <p>Querying</p> <ul> <li>What: The process of sending a query to the system and getting a response based on the retrieved data and LLM's generation.</li> <li>Why: LlamaIndex facilitates structured querying of indexed data to obtain relevant information and generate insights.</li> </ul>"},{"location":"foundations/tools_overview/#llamaindex-for-building-company-apps","title":"LlamaIndex for Building Company Apps","text":""},{"location":"foundations/tools_overview/#use-case-internal-knowledge-base-for-a-company","title":"Use Case: Internal Knowledge Base for a Company","text":"<p>Scenario: You're building an internal app for a company that allows employees to query and get instant answers based on the company's documentation, knowledge base, product manuals, and other internal resources (e.g., HR policies, code documentation). The system should automatically retrieve the most relevant information and generate responses using the context from that data.</p>"},{"location":"foundations/tools_overview/#how-llamaindex-helps","title":"How LlamaIndex Helps:","text":"<p>LlamaIndex simplifies the process of indexing and retrieving information from the company's data sources, which can then be combined with an LLM (like GPT) to generate context-aware responses. Here's how:</p>"},{"location":"foundations/tools_overview/#steps-with-llamaindex","title":"Steps with LlamaIndex","text":"<p>Connect Data Sources (Data Connectors)</p> <ul> <li>Integrate data from multiple sources such as:</li> <li>Internal databases (e.g., SQL, NoSQL).</li> <li>Document repositories (e.g., PDFs, Word documents, Google Docs).</li> <li>Knowledge management systems (e.g., Confluence, Notion).</li> <li>LlamaIndex connects to these sources and pulls the data needed for indexing.</li> </ul> <p>Index the Data</p> <ul> <li>LlamaIndex creates an index for all your documents, data, and knowledge base to make it efficient for quick retrieval.</li> <li>The indexed data is structured in a way that allows fast searches and contextual understanding by the LLM.</li> </ul> <p>Retrieve Information (Retrievers)</p> <ul> <li>When an employee submits a query, LlamaIndex uses its retriever components to search through the indexed data and fetch the most relevant information (e.g., product specs, HR policies, troubleshooting guides).</li> </ul> <p>Query the LLM for Responses</p> <ul> <li>After retrieving relevant information from the data sources, the system sends it to an LLM (e.g., GPT) to generate a contextual response.</li> <li>The model uses the data to summarize, provide direct answers, or offer insights.</li> </ul> <p>Provide the Answer to the User</p> <ul> <li>The employee gets a response based on the context provided by the indexed data and the language model's generation capabilities.</li> </ul>"},{"location":"foundations/tools_overview/#example-hr-policy-querying-system","title":"Example: HR Policy Querying System","text":"<p>Imagine you are building a HR Query Assistant app for employees.</p>"},{"location":"foundations/tools_overview/#step-by-step-process-with-llamaindex","title":"Step-by-Step Process with LlamaIndex:","text":"<ol> <li>Data Sources:</li> <li>HR Policies Database: Contains documents on leave policies, health benefits, company rules, and more.</li> <li>Employee Handbook: A PDF document detailing work hours, company culture, code of conduct, etc.</li> <li> <p>FAQ page: Frequently asked questions about various HR-related topics.</p> </li> <li> <p>Indexing:</p> </li> <li> <p>LlamaIndex indexes all these data sources, ensuring efficient searching and retrieval when queries are made.</p> </li> <li> <p>Retrieving Data:</p> </li> <li>An employee might ask the app, \"How many sick leave days am I entitled to?\"</li> <li> <p>The retriever in LlamaIndex finds the relevant parts of the HR policy mentioning sick leave.</p> </li> <li> <p>Generating Contextual Responses:</p> </li> <li>LlamaIndex sends the retrieved information to the LLM. The LLM generates a response like:<ul> <li>\"You are entitled to 10 days of paid sick leave per year, according to the company HR policy.\"</li> </ul> </li> <li> <p>The model can also provide additional context if necessary, like the process for submitting a sick leave request.</p> </li> <li> <p>Providing the Answer:</p> </li> <li>The employee sees the answer instantly, without needing to dig through documents themselves.</li> </ol>"},{"location":"foundations/tools_overview/#business-benefits-of-using-llamaindex","title":"Business Benefits of Using LlamaIndex","text":"<p>Efficiency:</p> <ul> <li>Fast responses: Employees can instantly get answers based on up-to-date, indexed knowledge.</li> <li>Reduced manual effort: No more manually searching through documents or waiting for HR reps to answer questions.</li> </ul> <p>Accuracy:</p> <ul> <li>Contextual understanding: LlamaIndex ensures that the language model understands the context and gives accurate, data-backed responses.</li> <li>Consistent answers: The model provides consistent and reliable answers derived directly from the company's data sources.</li> </ul> <p>Scalability:</p> <ul> <li>As the company grows, the knowledge base can also grow. LlamaIndex can handle indexing and retrieving information from an expanding set of data sources with little additional effort.</li> </ul> <p>Real-Time Information:</p> <ul> <li>Any updates made to documents or databases can be immediately indexed, ensuring the system always provides the latest information to employees.</li> </ul> <p>By integrating LlamaIndex into your app, you can create intelligent, real-time, and contextually aware systems for your company's internal use. Whether it's for answering HR policy questions, providing troubleshooting guides for IT, or summarizing legal documents, LlamaIndex gives you the power to connect your company's knowledge base to an LLM, enabling faster, more accurate responses.</p> <p>This integration ensures that employees can access the information they need when they need it, saving time and improving productivity across the organization.</p>"},{"location":"foundations/tools_overview/#vector-databases","title":"Vector Databases","text":"<p>Vector DBs store embeddings (numerical representations of text/code) and allow fast similarity search.</p> <ul> <li> <p>Why we need them:   LLMs have limited context windows. Vector DBs help find the most relevant info based on meaning, not just keyword match.</p> </li> <li> <p>Use Case:   Searching internal docs, logs, or knowledge base for the closest relevant information to user queries.</p> </li> </ul>"},{"location":"foundations/tools_overview/#common-vector-dbs","title":"Common Vector DBs","text":"<p>FAISS</p> <ul> <li>Developed by Facebook.</li> <li>Fast, runs locally, no cloud needed.</li> <li>Best for local dev and prototyping.</li> </ul> <p>Chroma</p> <ul> <li>Open-source, Python-native, easy to integrate with LangChain.</li> <li>Supports persistence (saves data across sessions).</li> </ul> <p>Pinecone</p> <ul> <li>Fully managed, scalable, and production-ready.</li> <li>Great for apps with large-scale retrieval (millions of records).</li> </ul>"},{"location":"foundations/tools_overview/#how-vector-dbs-are-different-from-sqlnosql-dbs","title":"How Vector DBs are different from SQL/NoSQL DBs","text":"<p>Traditional databases (SQL or NoSQL) store and query structured data using exact matches, filters, and joins.</p> <ul> <li>Example:   Find all users where <code>role = 'admin'</code> or <code>created_at &gt; 2024-01-01</code>.</li> </ul> <p>Vector databases store and query unstructured data (text, code, images) using embeddings and semantic similarity.</p> <ul> <li>Example:   Find the doc chunk most similar in meaning to: \u201cHow does user login work?\u201d</li> </ul> Feature SQL / NoSQL DB Vector DB Data Type Structured (rows, docs) Unstructured (text, code) Query Type Exact match, filter Similarity search Use Case CRUD apps, reports LLMs, semantic search Indexing Columns, fields Embedding vectors"},{"location":"foundations/tools_overview/#do-you-need-both-databases","title":"Do You Need Both Databases?","text":"<p>Yes \u2014 in most real-world LLM apps, you often use both:</p> <ol> <li> <p>Traditional DB:    Store user profiles, logs, metadata, preferences, etc.</p> </li> <li> <p>Vector DB:    Store embeddings of documents, messages, or code for semantic retrieval.</p> </li> <li> <p>Example App: AI Assistant for your company:</p> <ul> <li>SQL DB: Stores users, chat history, permissions.</li> <li>Vector DB: Stores indexed knowledge base, technical docs, internal wikis.</li> </ul> </li> </ol> <p>So they serve different roles, and together enable powerful, intelligent apps.</p>"}]}